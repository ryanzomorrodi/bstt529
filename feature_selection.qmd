

## Feature Selection Methods
### Filter Methods

**Information gain** - Calculates how well a given feature is able to reduce entropy.

### Embedded

**Boruta** - Shuffles the values of each feature, creating "shadow features". Fits a random forest with the original features and "shadow features". If an original feature's importance is greater than that of the maximum "shadow feature", then it is labeled important. Does this many times until not features are added.
**Random Forest**
**Elastic Net**
**XGBoost**

```{r}
library(tidyverse)
library(tidymodels)
library(finetune)
library(themis)
library(colino)
library(gt)
```

```{r}
df <- arrow::read_parquet("data/depression.parquet")
```

```{r}
df_preped <- recipe(df, depression ~ .) %>%
    step_select(-SEQN) %>%
    step_filter(veteran == "Yes") %>%
    step_zv(all_predictors()) %>%
    step_impute_bag(
        seed_val = 123,
        c(where(~ any(is.na(.x))), -ends_with("cancer_type")),
        impute_with = imp_vars(-c(starts_with("Rx"), ends_with("cancer_type")))
    ) %>%
    prep() %>%
    juice()
```

```{r}
set.seed(234)
df_folds <- vfold_cv(df_preped, strata = depression, repeats = 5)
```

```{r}
rec_def <- recipe(df_preped, depression ~ .) %>%
    step_nzv(all_predictors()) %>%
    step_dummy_multi_choice(ends_with("cancer_type")) %>%
    step_dummy(all_factor_predictors()) %>%
    step_zv(all_predictors()) %>%
    step_smote(depression)

rec_nor <- rec_def %>%
    step_normalize(all_predictors())
```

```{r}
library(bonsai)

spec_lr <- logistic_reg(
    penalty = tune(),
    mixture = tune()
) %>%
    set_engine("glmnet") %>%
    set_mode("classification")

spec_rf <- rand_forest(
    mtry = tune(),
    trees = 1000,
    min_n = tune()
) %>%
    set_engine("ranger", importance = "impurity") %>%
    set_mode("classification")

spec_xg <- boost_tree(
    mtry = tune(),
    trees = 1000,
    min_n = tune(),
    tree_depth = tune(),
    learn_rate = tune(),
    loss_reduction = tune(),
    sample_size = tune(),
) %>%
    set_engine("lightgbm") %>%
    set_mode("classification")
```

```{r}
wflw_set <- workflow_set(
    preproc = list(normalization = rec_nor, default = rec_def),
    models = list(elastic_net = spec_lr, random_forest = spec_rf),
    cross = FALSE
)

wflw_set <- workflow_set(
    preproc = list(default = rec_def),
    models = list(light_gbm = spec_xg)
)
```

```{r}
race_ctrl <- control_race(
    parallel_over = "everything",
    save_workflow = TRUE,
    verbose = TRUE
)

results <- wflw_set %>%
    workflow_map(
        "tune_race_anova",
        seed = 345,
        resamples = df_folds,
        grid = 20,
        verbose = TRUE,
        control = race_ctrl
    )
```

```{r}
best_res_lr <- results %>% 
    extract_workflow_set_result("normalization_elastic_net") %>% 
    select_best(metric = "roc_auc")

best_wflw_lr <- results %>% 
    extract_workflow("normalization_elastic_net") %>% 
    finalize_workflow(best_res_lr)

imp_lr <- best_wflw_lr %>%
    fit(df_preped) %>%
    extract_fit_parsnip() %>%
    pull_importances()
```

```{r}
best_res_rf <- results %>% 
    extract_workflow_set_result("default_random_forest") %>% 
    select_best(metric = "roc_auc")

best_wflw_rf <- results %>% 
    extract_workflow("default_random_forest") %>% 
    finalize_workflow(best_res_rf)

imp_rf <- best_wflw_rf %>%
    fit(df_preped) %>%
    extract_fit_parsnip() %>%
    pull_importances()
```

```{r}
best_res_xg <- results %>% 
    extract_workflow_set_result("default_light_gbm") %>% 
    select_best(metric = "roc_auc")

best_wflw_xg <- results %>% 
    extract_workflow("default_light_gbm") %>% 
    finalize_workflow(best_res_xg)

pull_importances._lgb.Booster <- function(
    object,
    scaled = TRUE,
    type = "Gain",
    ...) {

    call <- rlang::call2(
        .fn = "lgb.importance",
        .ns = "lightgbm",
        model = object$fit
    )
    scores <- rlang::eval_tidy(call)
    scores <- tibble(feature = scores$Feature, importance = scores[[type]])

    if (scaled)
        scores$importance <- rescale(scores$importance)

    scores
}

imp_rf <- best_wflw_xg %>%
    fit(df_preped) %>%
    extract_fit_parsnip() %>%
    pull_importances()
```
